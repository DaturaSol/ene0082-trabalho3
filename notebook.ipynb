{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7903e822",
   "metadata": {},
   "source": [
    "# Trabalho Computacional 3. Rede Convolucional e Transfer Learning\n",
    "\n",
    "> **Nome**: *Gabriel Martins Silveira de Oliveira*.  \n",
    "> **Matrícula**: 190042656.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ccdc85",
   "metadata": {},
   "source": [
    "# Google colab & imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543d007f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA detected in environment!\n",
      "Configuring for gpu\n",
      "Using 1 Device(s).\n",
      "Using 8 Worker(s)\n",
      "With Batch Size 64.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import jax\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "accelerator_type = jax.default_backend()\n",
    "num_devices = jax.local_device_count()\n",
    "num_workers = os.cpu_count()\n",
    "batch_size = 64\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab\")\n",
    "    try:\n",
    "        subprocess.run(\n",
    "            [\"pip\", \"install\", \"pytorch-lightning\", \"torchmetrics\"], check=True\n",
    "        )\n",
    "        print(\"Successfully installed packages.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error installing packages: {e}\")\n",
    "      \n",
    "if accelerator_type == \"tpu\":\n",
    "    print(\"TPU detected in environment!\")\n",
    "    \n",
    "    batch_size = batch_size * num_devices\n",
    "    print(\n",
    "        f\"Configuring for {accelerator_type}\\n\"\n",
    "        f\"Using {num_devices} Device(s).\\n\"\n",
    "        f\"Using {num_workers} Worker(s)\\n\"\n",
    "        f\"With Batch Size {batch_size}.\"\n",
    "    )\n",
    " \n",
    "if accelerator_type == \"gpu\":\n",
    "    print(\"GPU detected in environment!\")\n",
    "    print(\n",
    "        f\"Configuring for {accelerator_type}\\n\"\n",
    "        f\"Using {num_devices} Device(s).\\n\"\n",
    "        f\"Using {num_workers} Worker(s)\\n\"\n",
    "        f\"With Batch Size {batch_size}.\"\n",
    "    )\n",
    "   \n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA detected in environment!\")\n",
    "    accelerator_type = \"gpu\"\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    print(\n",
    "        f\"Configuring for {accelerator_type}\\n\"\n",
    "        f\"Using {num_devices} Device(s).\\n\"\n",
    "        f\"Using {num_workers} Worker(s)\\n\"\n",
    "        f\"With Batch Size {batch_size}.\"\n",
    "    )\n",
    "       \n",
    "if accelerator_type == \"cpu\":\n",
    "    print(\"No TPU or GPU detected. Using CPU.\")\n",
    "    print(\n",
    "        f\"Configuring for {accelerator_type}\\n\"\n",
    "        f\"Using {num_devices} Device(s).\\n\"\n",
    "        f\"Using {num_workers} Worker(s)\\n\"\n",
    "        f\"With Batch Size {batch_size}.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01051123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "import torch.nn as nn\n",
    "from torch import optim, Generator\n",
    "from torch.utils.data import random_split, Subset, DataLoader\n",
    "\n",
    "# TorchVision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import (\n",
    "    vgg16, \n",
    "    VGG16_Weights, \n",
    "    inception_v3, \n",
    "    Inception_V3_Weights,\n",
    ")\n",
    "\n",
    "# TorchMetrics\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "# TorchLightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "# Numpy\n",
    "import numpy as np\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.axes import Axes\n",
    "\n",
    "# Utils\n",
    "from pathlib import Path\n",
    "from argparse import Namespace\n",
    "from typing import List, Tuple, cast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988c1a8e",
   "metadata": {},
   "source": [
    "## Base de Dados\n",
    "\n",
    "### A base de dados é a [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html).  \n",
    "\n",
    "Ela contém 60000 imagens 32x32 coloridas (3 canais) das seguintes categorias de objetos: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d4484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10DataSet():   \n",
    "    train: Subset[datasets.CIFAR10]\n",
    "    val: Subset[datasets.CIFAR10]\n",
    "    test: datasets.CIFAR10\n",
    "    classes: List[str]\n",
    "    resize: Tuple[int, int]\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        root: Path, \n",
    "        resize: Tuple[int, int]=(224, 224),\n",
    "    ):  \n",
    "        trans = transforms.Compose([\n",
    "            transforms.Resize(resize),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                (0.5, 0.5, 0.5), \n",
    "                (0.5, 0.5, 0.5),\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        self.full_train = datasets.CIFAR10(root=root, train=True, transform=trans, download=True)\n",
    "        \n",
    "        self.classes = self.full_train.classes\n",
    "        \n",
    "        train_set_size = int(len(self.full_train) * 0.8)\n",
    "        valid_set_size = len(self.full_train) - train_set_size\n",
    "        \n",
    "        seed = Generator().manual_seed(42)\n",
    "        self.train, \\\n",
    "        self.val = random_split(\n",
    "            self.full_train, \n",
    "            [train_set_size, valid_set_size], \n",
    "            generator=seed\n",
    "        )\n",
    "        \n",
    "        self.test = datasets.CIFAR10(root=root, train=False, transform=trans, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b36d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(\"./.data/\")\n",
    "root.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc65044",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CIFAR10DataSet(root=root)\n",
    "\n",
    "if not num_workers:\n",
    "    num_workers = 0\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset.train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    dataset.val,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    dataset.test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Number of training examples: {len(dataset.train)}\\n\"\n",
    "    f\"Number of validation examples: {len(dataset.val)}\\n\"\n",
    "    f\"Number of test examples: {len(dataset.test)}\\n\\n\"\n",
    "    f\"Number of Classes: {len(dataset.classes)}\\n\"\n",
    "    \"Classes:\",\n",
    "    *dataset.classes,\n",
    "    sep=\"\\n  \",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4a82b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_batch(\n",
    "    images_batch: torch.Tensor,\n",
    "    labels_batch: torch.Tensor,\n",
    "    class_names: List[str],\n",
    "    mean: float = 0.5,\n",
    "    std: float = 0.5,\n",
    "    num_images_to_show: int = 9,\n",
    "):\n",
    "    images_batch = images_batch.cpu()\n",
    "\n",
    "    axes_grid_array: np.ndarray\n",
    "\n",
    "    _, axes_grid_array = plt.subplots(\n",
    "        3, 3, figsize=(10, 10)\n",
    "    )\n",
    "\n",
    "    axes_flat_array = axes_grid_array.flatten()\n",
    "\n",
    "    for i in range(min(num_images_to_show, len(images_batch))):\n",
    "        ax = axes_flat_array[i]\n",
    "        assert isinstance(ax, Axes), \"Each element should be an Axes object\" # Pylance...\n",
    "\n",
    "        # We need to unnormalize the image ):\n",
    "        img = images_batch[i]\n",
    "        img = img * std + mean\n",
    "        img = img.permute(1, 2, 0)\n",
    "        img = torch.clamp(img, 0, 1)\n",
    "        \n",
    "        ax.imshow(img.numpy())\n",
    "        label_index: int = int(labels_batch[i].item())\n",
    "        ax.set_title(f\"Label: {class_names[label_index]}\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e1d07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(train_dataloader)\n",
    "try:\n",
    "    images, labels = next(data_iter)\n",
    "    \n",
    "    print(\"Displaying a batch of training images after transforms:\")\n",
    "    imshow_batch(images, labels, dataset.classes, num_images_to_show=9)\n",
    "\n",
    "except StopIteration:\n",
    "    print(\"DataLoader is empty or could not fetch a batch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fafcb74",
   "metadata": {},
   "source": [
    "# Treinando um MLP\n",
    "\n",
    "Use esta base de dados para treinar um Perceptron Multicamadas, como feito no trabalho anterior com a base MNIST. Escolha um MLP com 2 camadas escondidas. Não perca muito tempo variando a arquitetura porque este problema é difícil sem o uso de convoluções e o resultado não será totalmente satisfatório.\n",
    "\n",
    "Você pode usar este código, baseado na biblioteca [Pytorch Lightning](https://lightning.ai/docs/pytorch/stable/) como base para definição da rede:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6b7f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelHParams(Namespace):\n",
    "    lr: float\n",
    "    num_classes: int\n",
    "    weight_decay: float  # L2 Regularization\n",
    "    l1_strength: float  # L1 Regularization\n",
    "\n",
    "\n",
    "class LightModel(pl.LightningModule):\n",
    "    model: nn.Module\n",
    "    hparams: ModelHParams\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        lr: float = 1e-3,\n",
    "        num_classes: int = 10,\n",
    "        weight_decay: float = 0.0,  # No L2 regularization\n",
    "        l1_strength: float = 0.0,  # No L1 regularization\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=[\"model\"])\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:\n",
    "        X, y = batch\n",
    "        y_hat: torch.Tensor = self(X)\n",
    "        cross_entropy_loss = nn.functional.cross_entropy(y_hat, y)\n",
    "        total_loss = cross_entropy_loss\n",
    "\n",
    "        if self.hparams.l1_strength > 0:\n",
    "            total_loss = self._deal_with_l1(total_loss)\n",
    "            self.log(\n",
    "                \"train_cross_entropy_loss\",\n",
    "                cross_entropy_loss,\n",
    "                on_step=True,\n",
    "                on_epoch=True,\n",
    "                prog_bar=False,\n",
    "            )\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            total_loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "        return total_loss\n",
    "\n",
    "    def _deal_with_l1(self, total_loss: torch.Tensor) -> torch.Tensor:\n",
    "        l1_penalty = 0.0\n",
    "        for param in self.model.parameters():\n",
    "            if param.requires_grad:\n",
    "                l1_penalty += torch.norm(param, 1)  # L1 norm\n",
    "        total_loss += self.hparams.l1_strength * l1_penalty\n",
    "        return total_loss\n",
    "\n",
    "    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:\n",
    "        X, y = batch\n",
    "        y_hat: torch.Tensor = self(X)\n",
    "        loss = nn.functional.cross_entropy(y_hat, y)\n",
    "        self.log(\n",
    "            \"val_loss\",\n",
    "            loss,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch: Tuple[torch.Tensor, torch.Tensor]):\n",
    "        X, y = batch\n",
    "        y_hat: torch.Tensor = self(X)\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        acc = accuracy(\n",
    "            preds,\n",
    "            y,\n",
    "            task=\"multiclass\",\n",
    "            num_classes=self.hparams.get(\"num_classes\", 10),\n",
    "        )\n",
    "        self.log(\"test_acc\", acc, on_epoch=True)\n",
    "        loss = nn.functional.cross_entropy(y_hat, y)\n",
    "        self.log(\"test_loss\", loss, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self) -> optim.Optimizer:\n",
    "        optimizer = optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            weight_decay=self.hparams.weight_decay,\n",
    "        )\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931c34ad",
   "metadata": {},
   "source": [
    "## Sem L1 e L2 ou droput Basico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79df29bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(3 * 224 * 224, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 10),\n",
    ")\n",
    "\n",
    "mlp = LightModel(arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1813ae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5, \n",
    "    mode=\"min\",\n",
    "    min_delta=0.001,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    callbacks=[early_stopping], max_epochs=50, accelerator=accelerator_type, devices=num_devices\n",
    ")\n",
    "print(\"\\n--- Training MLP classifier ---\")\n",
    "trainer.fit(\n",
    "    model=mlp, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b19b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Testing MPL classifier ---\")\n",
    "trainer.test(model=mlp, dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2016cf",
   "metadata": {},
   "source": [
    "## Apenas L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561c39f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = LightModel(arch, l1_strength=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c28051",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5, \n",
    "    mode=\"min\",\n",
    "    min_delta=0.001,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    callbacks=[early_stopping], max_epochs=50, accelerator=accelerator_type, devices=num_devices\n",
    ")\n",
    "print(\"\\n--- Training MLP classifier ---\")\n",
    "trainer.fit(\n",
    "    model=mlp, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aedd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Testing MPL classifier ---\")\n",
    "trainer.test(model=mlp, dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de4c87f",
   "metadata": {},
   "source": [
    "## Apenas L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8b8bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = LightModel(arch, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb66886",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5, \n",
    "    mode=\"min\",\n",
    "    min_delta=0.001,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    callbacks=[early_stopping], max_epochs=50, accelerator=accelerator_type, devices=num_devices\n",
    ")\n",
    "print(\"\\n--- Training MLP classifier ---\")\n",
    "trainer.fit(\n",
    "    model=mlp, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfde731",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Testing MPL classifier ---\")\n",
    "trainer.test(model=mlp, dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613b0d0d",
   "metadata": {},
   "source": [
    "## L1 e L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0f4ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = LightModel(arch, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c301576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5, \n",
    "    mode=\"min\",\n",
    "    min_delta=0.001,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    callbacks=[early_stopping], max_epochs=50, accelerator=accelerator_type, devices=num_devices\n",
    ")\n",
    "print(\"\\n--- Training MLP classifier ---\")\n",
    "trainer.fit(\n",
    "    model=mlp, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136729b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Testing MPL classifier ---\")\n",
    "trainer.test(model=mlp, dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e118736",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc57e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_p = 0.3\n",
    "arch = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(3 * 224 * 224, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=dropout_p),\n",
    "    nn.Linear(64, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=dropout_p),\n",
    "    nn.Linear(16, 10),\n",
    ")\n",
    "\n",
    "mlp = LightModel(arch, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b60e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5, \n",
    "    mode=\"min\",\n",
    "    min_delta=0.001,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    callbacks=[early_stopping], max_epochs=50, accelerator=accelerator_type, devices=num_devices\n",
    ")\n",
    "print(\"\\n--- Training MLP classifier ---\")\n",
    "trainer.fit(\n",
    "    model=mlp, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4826416c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Testing MPL classifier ---\")\n",
    "trainer.test(model=mlp, dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ae0656",
   "metadata": {},
   "source": [
    "## Uso da rede VGG16 pré-treinada\n",
    "\n",
    "Lembre-se que a rede VGG usa como bloco básico cascata de convoluções com filtros 3x3, com \"padding\" para que a imagem não seja\n",
    "diminuída, seguida de um \"max pooling\" reduzindo imagens pela metade. O número de mapas vai aumentando e seu tamanho vai diminuindo\n",
    "ao longo de suas 16 camadas. Este é um modelo gigantesco e o treinamento com recursos computacionais modestos levaria dias ou\n",
    "semanas, se é que fosse possível.\n",
    "\n",
    "No entanto, vamos aproveitar uma característica central das grandes redes convolucionais. Elas podem ser usadas como pré-processamento\n",
    "fixo das imagens, mesmo em um novo problema (lembre-se, a rede VGG original foi treinada na base ImageNet, que tem muitas categorias de\n",
    "imagens).\n",
    "\n",
    "O código abaixo realiza o download do modelo treinado e configura os seus parâmetros como não ajustáveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebb58ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg = vgg16(weights=VGG16_Weights.DEFAULT, progress=True)\n",
    "\n",
    "for param in model_vgg.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088f87ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d8821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vgg_output_features = cast(int, model_vgg.classifier[0].in_features)  # Pylance..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedbe8f5",
   "metadata": {},
   "source": [
    "## Basico sem L1 e L2 ou droupout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342d6146",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_custom_classifier = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(num_vgg_output_features, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 10)\n",
    ")\n",
    "model_vgg.classifier = new_custom_classifier\n",
    "\n",
    "light_vgg_model = LightModel(model=model_vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efa32b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    mode=\"min\",\n",
    "    min_delta=0.001,\n",
    ")\n",
    "\n",
    "\n",
    "trainer_for_vgg = Trainer(\n",
    "    callbacks=[early_stopping], max_epochs=50, accelerator=accelerator_type, devices=num_devices\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n--- Training VGG16 with custom classifier ---\")\n",
    "trainer_for_vgg.fit(\n",
    "    model=light_vgg_model,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12ebf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Testing VGG16 with custom classifier ---\")\n",
    "trainer_for_vgg.test(model=light_vgg_model, dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eb12da",
   "metadata": {},
   "source": [
    "## L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9172bff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "light_vgg_model = LightModel(model=model_vgg, l1_strength=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd831d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    mode=\"min\",\n",
    "    min_delta=0.001,\n",
    ")\n",
    "\n",
    "\n",
    "trainer_for_vgg = Trainer(\n",
    "    callbacks=[early_stopping], max_epochs=50, accelerator=accelerator_type, devices=num_devices\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n--- Training VGG16 with custom classifier ---\")\n",
    "trainer_for_vgg.fit(\n",
    "    model=light_vgg_model,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e381b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Testing VGG16 with custom classifier ---\")\n",
    "trainer_for_vgg.test(model=light_vgg_model, dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb51ac52",
   "metadata": {},
   "source": [
    "## L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88441e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "light_vgg_model = LightModel(model=model_vgg, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb5a8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    mode=\"min\",\n",
    "    min_delta=0.001,\n",
    ")\n",
    "\n",
    "\n",
    "trainer_for_vgg = Trainer(\n",
    "    callbacks=[early_stopping], max_epochs=50, accelerator=accelerator_type, devices=num_devices\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n--- Training VGG16 with custom classifier ---\")\n",
    "trainer_for_vgg.fit(\n",
    "    model=light_vgg_model,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb50bff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Testing VGG16 with custom classifier ---\")\n",
    "trainer_for_vgg.test(model=light_vgg_model, dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2ba5bc",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cba98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_p = 0.3\n",
    "new_custom_classifier = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(num_vgg_output_features, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=dropout_p),\n",
    "    nn.Linear(64, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=dropout_p),\n",
    "    nn.Linear(16, 10)\n",
    ")\n",
    "model_vgg.classifier = new_custom_classifier\n",
    "\n",
    "light_vgg_model = LightModel(model=model_vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0500d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    mode=\"min\",\n",
    "    min_delta=0.001,\n",
    ")\n",
    "\n",
    "\n",
    "trainer_for_vgg = Trainer(\n",
    "    callbacks=[early_stopping], max_epochs=50, accelerator=accelerator_type, devices=num_devices\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n--- Training VGG16 with custom classifier ---\")\n",
    "trainer_for_vgg.fit(\n",
    "    model=light_vgg_model,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ae134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Testing VGG16 with custom classifier ---\")\n",
    "trainer_for_vgg.test(model=light_vgg_model, dataloaders=test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
